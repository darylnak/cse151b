{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashion_mnist_dataset.utils import mnist_reader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Returns the min-max normalized data.\n",
    "    Parameters:  \n",
    "                data: array_like\n",
    "                    Data to be normalized.\n",
    "    Returns:\n",
    "                normData: ndarray\n",
    "                    Normalized data.\n",
    "    \"\"\"\n",
    "    minVal = np.amin(data)\n",
    "    maxVal = np.amax(data)\n",
    "    \n",
    "    normData = data - minVal\n",
    "    normData = normData/(maxVal - minVal)\n",
    "    \n",
    "    return normData\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Returns the one-hot encoding for a set of labels.\n",
    "    Parameters:  \n",
    "                y: array_like\n",
    "                    Labels to be one-hot encoded.\n",
    "    Returns:\n",
    "                oneHot: ndarray\n",
    "                    One-hot encoded labels that is numData X numLabels.\n",
    "    \"\"\"\n",
    "    oneHot  = []\n",
    "    numCols = 0\n",
    "    labels  = np.unique(y.flatten())\n",
    "\n",
    "    mapping = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        mapping[label] = i\n",
    "    \n",
    "    if labels.size <= 2:\n",
    "        for label in y:\n",
    "            oneHot.append(mapping[label])\n",
    "    else:\n",
    "        for label in y:\n",
    "            encoding = list(np.zeros(labels.size, dtype=int))\n",
    "            encoding[mapping[label]] = 1\n",
    "            oneHot.append(encoding)\n",
    "    \n",
    "    return np.vstack(oneHot)\n",
    "\n",
    "def shuffle(data, labels):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Shuffle the data while maintaining proper labeling.\n",
    "    Parameters: \n",
    "                data: array_like\n",
    "                    Data to be shuffled.\n",
    "                labels: array_like\n",
    "                    Labels for each data point.\n",
    "    Returns:\n",
    "                dataShuffle: ndarray\n",
    "                    The shuffled data.\n",
    "                labelShuffle: ndarray\n",
    "                    The proper labels for the shuffled data.\n",
    "    \"\"\"\n",
    "    randIdxs = np.random.rand(data.shape[0]).argsort()\n",
    "    dataShuffle  = np.take(data, randIdxs, axis=0)\n",
    "    labelShuffle = np.take(labels, randIdxs, axis=0)\n",
    "    \n",
    "    return dataShuffle, labelShuffle\n",
    "\n",
    "def pca(data, comps):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Performs dimensionality reduction on data using Principle Component Analysis and returns the top comps PC's\n",
    "    Parameters:  \n",
    "                data: array_like\n",
    "                    Data to perform PCA on.\n",
    "                comps: int\n",
    "                    Number of components to return\n",
    "    Returns:\n",
    "                k_pcs: ndarray\n",
    "                    The top k principle components.\n",
    "    \"\"\"\n",
    "    # using procedure from slide 18: https://piazza.com/class_profile/get_resource/kfrlkk85pei2ma/kg1qafsrf721vt\n",
    "    mean = np.mean(data, axis=0)\n",
    "    A = data - mean\n",
    "    C = np.cov(A.T) # np.cov assumes columns are observations by default\n",
    "    \n",
    "    # returns e_vals in ascending order, e_vecs as column vectors\n",
    "    # we use eigh becuase C is symmetric (faster runtime)\n",
    "    e_vals, e_vecs = np.linalg.eigh(C)\n",
    "    k_pcs = np.flip(e_vecs[:,-comps:], axis=1) # flip columns to be in ascending order of eigen values\n",
    "\n",
    "    return k_pcs\n",
    "\n",
    "def getFolds(data, k):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Returns the indices from the data set of training and test data\n",
    "                for k-folds cross validation.\n",
    "    Parameters:  \n",
    "                data: array_like\n",
    "                   Data set to split.\n",
    "                k: int\n",
    "                    Number of folds.\n",
    "    Returns:\n",
    "                folds: list\n",
    "                    Contains a list of array pairs <train, test> that are the \n",
    "                    indicies from the data set for training and testing.\n",
    "    \"\"\"\n",
    "    folds = []\n",
    "    size = int(data.shape[0]/k) # size of each fold\n",
    "    \n",
    "    for i in range(0, data.shape[0], size):\n",
    "        test  = list(range(i, min(i+size, data.shape[0])))\n",
    "        train = list(range(i)) + list(range(i+size, data.shape[0]))\n",
    "        folds.append((train, test))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "def getData(X, y, classes):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Returns data for specific classes indicated from provided data.\n",
    "    Parameters:  \n",
    "                X: array_like\n",
    "                    The original dataset.\n",
    "                y: array_like\n",
    "                    The labels for each data point in the dataset.\n",
    "                classes: array_like\n",
    "                    Labels of the classes desired.\n",
    "    Returns:\n",
    "                new_data: array_like\n",
    "                    Data for the classes specified by classes.\n",
    "                new_labels: array_like (flat)\n",
    "                    Labels for the classes specified by classes.\n",
    "    \"\"\"\n",
    "    new_data   = []\n",
    "    new_labels = []\n",
    "\n",
    "    for label in classes:\n",
    "        # see the following link for more information: https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html\n",
    "        # .T[0] is because argwhere() returns a column vector and we need normal array to index data\n",
    "        idxs = np.argwhere(y==label).T[0]\n",
    "        new_data.append(X[idxs])\n",
    "        new_labels.append(y[idxs])\n",
    "    \n",
    "    new_data   = np.vstack(new_data)\n",
    "    new_labels = np.hstack(new_labels)\n",
    "\n",
    "    return new_data, new_labels\n",
    "\n",
    "def prepareData(X, k_pcs, fold=None, test=False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Prepares the data for training by performing k mutex splits, \n",
    "                dimensionality reduction using PCA, and adding a bias term to the data. \n",
    "    Parameters:  \n",
    "                X: array_like\n",
    "                    The original dataset.\n",
    "                k_pcs: array_like\n",
    "                    The top k principal components of the dataset.\n",
    "    Returns:\n",
    "                train_set: array_like\n",
    "                    Data ready for training.\n",
    "                val_set: array_like\n",
    "                    Data ready for validation testing.\n",
    "    \"\"\"\n",
    "    if test:\n",
    "        print(X.shape, \"prepare\")\n",
    "        test_set = X@k_pcs # get training, reduce using PCA\n",
    "        test_set = np.hstack((np.ones((test_set.shape[0], 1)), test_set)) # add bias\n",
    "        return test_set\n",
    "    \n",
    "    train_set = X[fold[0]]@k_pcs # get training, reduce using PCA\n",
    "    train_set = np.hstack(( np.ones((train_set.shape[0], 1)), train_set )) # add bias\n",
    "    \n",
    "    val_set   = X[fold[1]]@k_pcs # get validation, reduce using PCA\n",
    "    val_set   = np.hstack(( np.ones((val_set.shape[0], 1)), val_set )) # add bias\n",
    "    \n",
    "    return train_set, val_set\n",
    "\n",
    "def predictMulti(X, W):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Used for multiclass prediction.\n",
    "                Returns an array of predictions for data X with weights W. \n",
    "    Parameters:  \n",
    "                X: array_like\n",
    "                    Dataset to make predictions on.\n",
    "                W: array_like\n",
    "                    The weights of the model.\n",
    "    Returns:\n",
    "                pred: array_like.\n",
    "                    An N x C array of predictions for X, where N is the number of\n",
    "                    training examples and C is the number of classes.\n",
    "                G: array_like\n",
    "                    An N x C array array of probabilities for each class,\n",
    "                    where N is the number of training examples and C\n",
    "                    is the number of classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    A = X@W\n",
    "    G = np.exp(A)/np.sum(np.exp(A), axis=1).reshape(-1,1)\n",
    "    pred = np.zeros((X.shape[0], W.shape[1]))[ np.arange(X.shape[0]), np.argmax(G, axis=1) ] = 1\n",
    "    \n",
    "    return pred, G\n",
    "\n",
    "def predict(X, W):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Returns an array of predictions for data X with weights W. \n",
    "    Parameters:  \n",
    "                X: array_like\n",
    "                    Dataset to make predictions on.\n",
    "                W: array_like\n",
    "                    The weights of the model.\n",
    "    Returns:\n",
    "                pred: array_like.\n",
    "                    An array of predictions for X.\n",
    "                G: array_like\n",
    "                    Array of probabilities for each class.\n",
    "    \"\"\"\n",
    "    y_hat = []\n",
    "    \n",
    "    if W.shape[1] > 1:\n",
    "        return predictMulti(X, W)\n",
    "    \n",
    "    A = X@W\n",
    "    G = 1.0 / (np.exp(-A)+1)\n",
    "    #G = 1.0/(1.0+ np.exp(-A))\n",
    "    pred = np.asarray([1 if y >= 0.5 else 0 for y in G]) \n",
    "    \n",
    "    return pred, G\n",
    "\n",
    "def SGD(X, y, W, lr, b_size):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Estimates a new weights matrix W_new using Stochasitc Gradient Descent.\n",
    "    Parameters:  \n",
    "                X: array_like\n",
    "                    Training examples.\n",
    "                y: array_like\n",
    "                    Labels for each training example in X.\n",
    "                y_hat: array_like\n",
    "                    Probablities for each class for each training example in X.\n",
    "                W: array_like\n",
    "                    Current weights matrix for the model.\n",
    "                lr: int\n",
    "                    Learning rate\n",
    "                b_size:\n",
    "                    Batch size for mini-batching\n",
    "    Returns:\n",
    "                W_new: array_like.\n",
    "                     A new estimation of the weights matrix.\n",
    "    \"\"\"\n",
    "    W_new = W.copy()\n",
    "    randIdxs = np.random.rand(X.shape[0]).argsort()\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(0, X.shape[0], b_size):\n",
    "        start = i\n",
    "        end = min(i+b_size, X.shape[0])\n",
    "        \n",
    "        train_set = X[randIdxs[start:end]]\n",
    "        y_train = y[randIdxs[start:end]]\n",
    "        \n",
    "        _, y_hat_train = predict(train_set, W_new)\n",
    "\n",
    "        train_loss = crossEntropy(y_train, y_hat_train)\n",
    "        losses.append(train_loss)\n",
    "        \n",
    "        gradient = -( (y_train - y_hat_train).T@train_set )\n",
    "        W_new = W_new - lr*gradient.T\n",
    "        \n",
    "    return W_new, np.mean(losses)\n",
    "\n",
    "def crossEntropy(y, y_hat, multi=False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Computes the Cross-Entropy loss.\n",
    "    Parameters:  \n",
    "                y: array_like\n",
    "                    Labels for each training example in the dataset.\n",
    "                y_hat: array_like\n",
    "                    Probablities for each class for each training example in the dataset.\n",
    "    Returns:\n",
    "                loss: float.\n",
    "                     The loss.\n",
    "    \"\"\"\n",
    "    offset = 10**-10 # to prevent log(0)\n",
    "    \n",
    "    if y.shape[1] > 1:\n",
    "        loss = np.sum(y@np.log(y_hat+offset).T)\n",
    "        print(loss)\n",
    "    else:\n",
    "        loss = ((y.T@np.log(y_hat+offset) + (1-y).T@np.log(1-y_hat+offset))/y.shape[0]).flatten()[0]\n",
    "    \n",
    "    return -loss\n",
    "\n",
    "def plot(losses, titles, fold_num, set_name):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Plots the losses in losses with corresponding title from title.\n",
    "    Parameters:  \n",
    "                losses: list\n",
    "                    Losses to plot.\n",
    "                titles: list\n",
    "                    Title for each loss.\n",
    "    Returns:\n",
    "                Nothing\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    for i in range(len(losses)):\n",
    "        ax.plot( list(range(1, len(losses[i])+1)), losses[i], label=titles[i])\n",
    "        ax.set_title(set_name + \" Training and Validation Loss For Fold \" + str(fold_num))\n",
    "        ax.set(xlabel=\"Epoch\", ylabel=\"Loss\")\n",
    "    \n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def testModel(X, y, model, pcs, classes, is_test_set=True):\n",
    "    X_test = X\n",
    "    y_test = y\n",
    "    \n",
    "    if is_test_set:\n",
    "        X_test, y_test = getData(X, y, classes)\n",
    "        X_test = prepareData(X_test, pcs, test=True)\n",
    "        y_test = one_hot_encode(y_test)\n",
    "    \n",
    "    predictions, _ = predict(X_test, model)\n",
    "    total = y_test.flatten().shape[0]\n",
    "    numCorrect = abs(np.sum(y_test.flatten()==predictions.flatten()))\n",
    "    acc = numCorrect/total\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def initWeights(num_classes, p):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Save the weights matrix in a .npy binary file.\n",
    "    Parameters:  \n",
    "                W: array_like\n",
    "                    Weights matrix.\n",
    "                ID:\n",
    "                    File identification.\n",
    "    Returns:\n",
    "                A random weights matrix of size p+1 X 1 if binaray or\n",
    "                p+1 X num_classes if multiclass.\n",
    "    \"\"\"\n",
    "    return np.random.rand(p+1, 1) if num_classes <= 2 else np.random.rand(p+1, num_classes)\n",
    "    \n",
    "def savePCRecon(X):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Saves the figure for image reconstruction for multiple p PCs.\n",
    "    Parameters:  \n",
    "                X: array_like\n",
    "                    The dataset.\n",
    "    Returns:\n",
    "                None\n",
    "    \"\"\"\n",
    "    fig=plt.figure(figsize=(8, 5))\n",
    "    pcs = [2,10,50,100,200,784]\n",
    "    orig_img = X[0].reshape((1,784))\n",
    "    rows = 2\n",
    "    cols = 4\n",
    "    \n",
    "    ax = fig.add_subplot(rows,cols,1)\n",
    "    ax.title.set_text(\"Original\")\n",
    "    plt.imshow(orig_img.reshape((28,28)), cmap=\"gray\")\n",
    "    for i, pc in enumerate(pcs):\n",
    "        k_pcs = pca(X, comps=pc)\n",
    "        recon_img = (orig_img@k_pcs@k_pcs.T).reshape((28,28))\n",
    "        ax = fig.add_subplot(rows,cols,i+2)\n",
    "        ax.title.set_text(\"PCs: \" + str(pc))\n",
    "        plt.imshow(recon_img, cmap=\"gray\")\n",
    "\n",
    "    plt.savefig('./pc_reconstruction')\n",
    "\n",
    "def plotPCs(pcs, set_name):\n",
    "    fig=plt.figure(figsize=(5, 2))\n",
    "    rows = 2\n",
    "    cols = 5\n",
    "    \n",
    "    for i, pc in enumerate(pcs):\n",
    "        ax = fig.add_subplot(rows,cols,i+1)\n",
    "        ax.title.set_text(\"PC \" + str(i))\n",
    "        plt.imshow(pc.reshape((28,28)), cmap=\"gray\")\n",
    "    \n",
    "def logisticRegression(data, labels, classes, set_id):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                TODO Trains a prediction model using x_train, y_train\n",
    "    Parameters:  \n",
    "                data: array_like\n",
    "                    Training examples.\n",
    "                labels: array_like\n",
    "                    Labels of the training examples.\n",
    "    Returns:\n",
    "                model: array_like\n",
    "                    The best model from training used for prediction.\n",
    "    \"\"\"\n",
    "    LR = .001 # Learning rate\n",
    "    BS = 64 # Batch Size\n",
    "    p  = 350 # top p PC's\n",
    "    k  = 10   # number of folds\n",
    "    M  = 100 # number of epochs\n",
    "    loss  = crossEntropy # alias crossEntropy function\n",
    "    best_loss = np.inf\n",
    "    accs  = [] # accuracies for each fold\n",
    "    model = None # best model from training\n",
    "    loss_val   = []\n",
    "    loss_train = []\n",
    "    \n",
    "    X, y = getData(data, labels, classes)\n",
    "    X, y = shuffle(X, y)\n",
    "    y = one_hot_encode(y)\n",
    "\n",
    "    folds = getFolds(X, k=k)\n",
    "    k_pcs = pca(data, comps=p)\n",
    "    \n",
    "    savePCRecon(X)\n",
    "    \n",
    "    for i, fold in enumerate(tqdm(folds)):\n",
    "\n",
    "        train_set, val_set = prepareData(X, k_pcs, fold)\n",
    "        y_train = y[fold[0]]\n",
    "        y_val   = y[fold[1]]\n",
    "        \n",
    "        loss_train = []\n",
    "        loss_val   = []\n",
    "        \n",
    "        W = initWeights(len(classes), p)\n",
    "        print(y_val.shape)\n",
    "\n",
    "        for epoch in range(M):                  \n",
    "            W, train_loss = SGD(train_set, y_train, W, LR, BS)\n",
    "            loss_train.append(train_loss)\n",
    "            \n",
    "            _, y_hat_val = predict(val_set, W)\n",
    "            val_loss = loss(y_val, y_hat_val)\n",
    "            loss_val.append(val_loss)\n",
    "            \n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                model = W.copy()\n",
    "        \n",
    "        val_acc = testModel(val_set, y_val, model, k_pcs, classes, False)\n",
    "        accs.append(val_acc)\n",
    "        \n",
    "        set_name = \"\"\n",
    "        if set_id == 1:\n",
    "            set_name = \"T-shirt vs. Ankle Boot\"\n",
    "        elif set_id == 2:\n",
    "            set_name = \"Coat vs. Pullover\"\n",
    "        elif set_id == 3:\n",
    "            set_name = \"All Classes\"\n",
    "        \n",
    "        plot([loss_train, loss_val], [\"Training Loss\", \"Validation Loss\"], i+1, set_name)\n",
    "    \n",
    "    print(\"Average validation accuracy:\", np.mean(accs))        \n",
    "    \n",
    "    return model, k_pcs\n",
    "\n",
    "def softMaxRegression(data, labels, classes, set_id):\n",
    "    LR = .001 # Learning rate\n",
    "    BS = 64 # Batch Size\n",
    "    p  = 350 # top p PC's\n",
    "    k  = 10   # number of folds\n",
    "    M  = 100 # number of epochs\n",
    "    loss  = crossEntropy # alias crossEntropy function\n",
    "    best_loss = np.inf\n",
    "    accs  = [] # accuracies for each fold\n",
    "    model = None # best model from training\n",
    "    loss_val   = []\n",
    "    loss_train = []\n",
    "    \n",
    "    X, y = shuffle(data, labels)\n",
    "    y = one_hot_encode(y)\n",
    "\n",
    "    folds = getFolds(X, k=k)\n",
    "    k_pcs = pca(data, comps=p)\n",
    "    \n",
    "    for i, fold in enumerate(tqdm(folds)):\n",
    "\n",
    "        train_set, val_set = prepareData(X, k_pcs, fold)\n",
    "        y_train = y[fold[0]]\n",
    "        y_val   = y[fold[1]]\n",
    "        \n",
    "        loss_train = []\n",
    "        loss_val   = []\n",
    "        \n",
    "        W = initWeights(len(classes), p)\n",
    "\n",
    "        for epoch in range(M):                  \n",
    "            W, train_loss = SGD(train_set, y_train, W, LR, BS)\n",
    "            loss_train.append(train_loss)\n",
    "            \n",
    "            _, y_hat_val = predict(val_set, W)\n",
    "            val_loss = loss(y_val, y_hat_val)\n",
    "            loss_val.append(val_loss)\n",
    "            \n",
    "            if val_loss < best_loss:\n",
    "                best_loss = val_loss\n",
    "                model = W.copy()\n",
    "        \n",
    "        val_acc = testModel(val_set, y_val, model, k_pcs, classes, False)\n",
    "        accs.append(val_acc)\n",
    "        \n",
    "        set_name = \"\"\n",
    "        if set_id == 1:\n",
    "            set_name = \"T-shirt vs. Ankle Boot\"\n",
    "        elif set_id == 2:\n",
    "            set_name = \"Coat vs. Pullover\"\n",
    "        \n",
    "        plot([loss_train, loss_val], [\"Training Loss\", \"Validation Loss\"], i+1, set_name)\n",
    "    \n",
    "    print(\"Average validation accuracy:\", np.mean(accs))        \n",
    "    \n",
    "    return model, k_pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:15<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 11 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-27f5d47cdd37>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# classes we want for multiclass classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_pcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftMaxRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_pcs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7f4fef3627c1>\u001b[0m in \u001b[0;36msoftMaxRegression\u001b[0;34m(data, labels, classes, set_id)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m             \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m             \u001b[0mloss_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7f4fef3627c1>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(X, y, W, lr, b_size)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrossEntropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7f4fef3627c1>\u001b[0m in \u001b[0;36mcrossEntropy\u001b[0;34m(y, y_hat, multi)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 11 is different from 10)"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X_train, y_train = mnist_reader.load_mnist('fashion_mnist_dataset/data/fashion', kind='train')\n",
    "    X_test, y_test   = mnist_reader.load_mnist('fashion_mnist_dataset/data/fashion', kind='t10k')\n",
    "    \n",
    "    # min-max normalize\n",
    "    X_train = normalize(X_train)\n",
    "    X_test  = normalize(X_test)\n",
    "    \n",
    "    pcs = []\n",
    "    \n",
    "#     classes = [0, 9] # classes we want for binary classification\n",
    "#     model, k_pcs = logisticRegression(X_train, y_train, classes, 1)\n",
    "#     acc = testModel(X_test, y_test, model, k_pcs, classes)\n",
    "#     print(acc)\n",
    "#     pcs.append(k_pcs)\n",
    "    \n",
    "#     classes = [2,4]\n",
    "#     model, k_pcs = logisticRegression(X_train, y_train, classes, 2)\n",
    "#     acc = testModel(X_test, y_test, model, k_pcs, classes)\n",
    "#     print(acc)\n",
    "#     pcs.append(k_pcs)\n",
    "    \n",
    "    classes = list(range(11)) # classes we want for multiclass classification\n",
    "    model, k_pcs = softMaxRegression(X_train, y_train, classes, 3)\n",
    "    acc = testModel(X_test, y_test, model, k_pcs, classes)\n",
    "    print(acc)\n",
    "    pcs.append(k_pcs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
