{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashion_mnist_dataset.utils import mnist_reader\n",
    "import numpy as np\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Returns the min-max normalized data.\n",
    "    Parameters:  \n",
    "                data: array_like\n",
    "                    Data to be normalized.\n",
    "    Returns:\n",
    "                normData: ndarray\n",
    "                    Normalized data.\n",
    "    \"\"\"\n",
    "    minVal = np.amin(data)\n",
    "    maxVal = np.amax(data)\n",
    "    \n",
    "    normData = data - minVal\n",
    "    normData = normData/(maxVal - minVal)\n",
    "    \n",
    "    return normData\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Returns the one-hot encoding for a set of labels.\n",
    "    Parameters:  \n",
    "                labels: array_like\n",
    "                    Labels to be one-hot encoded.\n",
    "    Returns:\n",
    "                oneHot: ndarray\n",
    "                    One-hot encoded labels that is numData X numLabels.\n",
    "    \"\"\"\n",
    "    numLabels = np.amax(labels) - np.amin(labels) + 1\n",
    "    oneHot = np.zeros((labels.shape[0], numLabels))\n",
    "    \n",
    "    # See https://numpy.org/devdocs/user/basics.indexing.html#indexing-multi-dimensional-arrays\n",
    "    oneHot[np.array(range(labels.shape[0])), labels] = 1\n",
    "    \n",
    "    return oneHot\n",
    "\n",
    "def shuffle(data, labels):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Shuffle the data while maintaining proper labeling.\n",
    "    Parameters: \n",
    "                data: array_like\n",
    "                    Data to be shuffled.\n",
    "                labels: array_like\n",
    "                    Labels for each data point.\n",
    "    Returns:\n",
    "                dataShuffle: ndarray\n",
    "                    The shuffled data.\n",
    "                labelShuffle: ndarray\n",
    "                    The proper labels for the shuffled data.\n",
    "    \"\"\"\n",
    "    randIdxs = np.random.rand(data.shape[0]).argsort()\n",
    "    dataShuffle  = np.take(data, randIdxs, axis=0)\n",
    "    labelShuffle = np.take(labels, randIdxs, axis=0)\n",
    "    \n",
    "    return dataShuffle, labelShuffle\n",
    "\n",
    "def pca(data, k):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Performs dimensionality reduction on data using Principle Component Analysis and returns the top k components\n",
    "    Parameters:  \n",
    "                data: array_like\n",
    "                    Data to perform PCA on.\n",
    "                k: int\n",
    "                    Number of components to return\n",
    "    Returns:\n",
    "                dataReduc: ndarray\n",
    "                    The reduced dimensionality data.\n",
    "    \"\"\"\n",
    "    # using procedure from slide 18: https://piazza.com/class_profile/get_resource/kfrlkk85pei2ma/kg1qafsrf721vt\n",
    "    mean = np.mean(data, axis=0)\n",
    "    A = data - mean\n",
    "    C = np.cov(A.T) # np.cov assumes columns are observations by default\n",
    "    \n",
    "    # returns e_vals in ascending order, e_vecs as column vectors\n",
    "    # we use eigh becuase C is symmetric (faster)\n",
    "    e_vals, e_vecs = np.linalg.eigh(C)\n",
    "    k_pcomps = e_vecs[:,-k:]\n",
    "    dataReduc = data@k_pcomps\n",
    "\n",
    "    return dataReduc\n",
    "\n",
    "def train_model(train, labels):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                TODO Trains a prediction model using x_train, y_train\n",
    "    Parameters:  \n",
    "                train: array_like\n",
    "                    Training examples.\n",
    "                labels: array_like\n",
    "                    Labels of the training examples.\n",
    "    Returns:\n",
    "                TODO\n",
    "    \"\"\"\n",
    "    LR = .01 # Learning rate\n",
    "    BS = 512 # Batch Size\n",
    "    k  = 50\n",
    "    W  = np.rand((X_train.shape[0], k+1)) # weights matrix\n",
    "    X_train, y_train = shuffle(train, labels) \n",
    "    X_train = pca(train, k=k)\n",
    "    X_train = np.hstack((X_train, np.ones((X_train.shape[0], 1)))) # for bias term\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X_train, y_train = mnist_reader.load_mnist('fashion_mnist_dataset/data/fashion', kind='train')\n",
    "    X_test, y_test   = mnist_reader.load_mnist('fashion_mnist_dataset/data/fashion', kind='t10k')\n",
    "    \n",
    "    # min-max normalize\n",
    "    X_train = normalize(X_train)\n",
    "    X_test  = normalize(X_test)\n",
    "    # one-hot encode labels\n",
    "    y_train = one_hot_encode(y_train)\n",
    "    y_test  = one_hot_encode(y_test)\n",
    "    \n",
    "    train_model(X_train, y_train)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
