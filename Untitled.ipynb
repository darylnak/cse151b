{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fashion_mnist_dataset.utils import mnist_reader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalize(data):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Returns the min-max normalized data.\n",
    "    Parameters:  \n",
    "                data: array_like\n",
    "                    Data to be normalized.\n",
    "    Returns:\n",
    "                normData: ndarray\n",
    "                    Normalized data.\n",
    "    \"\"\"\n",
    "    minVal = np.amin(data)\n",
    "    maxVal = np.amax(data)\n",
    "    \n",
    "    normData = data - minVal\n",
    "    normData = normData/(maxVal - minVal)\n",
    "    \n",
    "    return normData\n",
    "\n",
    "def one_hot_encode(y):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Returns the one-hot encoding for a set of labels.\n",
    "    Parameters:  \n",
    "                y: array_like\n",
    "                    Labels to be one-hot encoded.\n",
    "    Returns:\n",
    "                oneHot: ndarray\n",
    "                    One-hot encoded labels that is numData X numLabels.\n",
    "    \"\"\"\n",
    "    oneHot  = []\n",
    "    numCols = 0\n",
    "    labels  = np.unique(y.T)\n",
    "\n",
    "    mapping = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        mapping[label] = i\n",
    "    \n",
    "    if labels.size <= 2:\n",
    "        for label in y:\n",
    "            oneHot.append(mapping[label])\n",
    "    else:\n",
    "        for label in y:\n",
    "            encoding = list(np.zeros(labels.size, dtype=int))\n",
    "            encoding[mapping[label]] = 1\n",
    "            oneHot.append(encoding)\n",
    "    \n",
    "    return np.vstack(oneHot)\n",
    "\n",
    "def shuffle(data, labels):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Shuffle the data while maintaining proper labeling.\n",
    "    Parameters: \n",
    "                data: array_like\n",
    "                    Data to be shuffled.\n",
    "                labels: array_like\n",
    "                    Labels for each data point.\n",
    "    Returns:\n",
    "                dataShuffle: ndarray\n",
    "                    The shuffled data.\n",
    "                labelShuffle: ndarray\n",
    "                    The proper labels for the shuffled data.\n",
    "    \"\"\"\n",
    "    randIdxs = np.random.rand(data.shape[0]).argsort()\n",
    "    dataShuffle  = np.take(data, randIdxs, axis=0)\n",
    "    labelShuffle = np.take(labels, randIdxs, axis=0)\n",
    "    \n",
    "    return dataShuffle, labelShuffle\n",
    "\n",
    "def pca(data, comps):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Performs dimensionality reduction on data using Principle Component Analysis and returns the top comps PC's\n",
    "    Parameters:  \n",
    "                data: array_like\n",
    "                    Data to perform PCA on.\n",
    "                comps: int\n",
    "                    Number of components to return\n",
    "    Returns:\n",
    "                k_pcs: ndarray\n",
    "                    The top k principle components.\n",
    "    \"\"\"\n",
    "    # using procedure from slide 18: https://piazza.com/class_profile/get_resource/kfrlkk85pei2ma/kg1qafsrf721vt\n",
    "    mean = np.mean(data, axis=0)\n",
    "    A = data - mean\n",
    "    C = np.cov(A.T) # np.cov assumes columns are observations by default\n",
    "    \n",
    "    # returns e_vals in ascending order, e_vecs as column vectors\n",
    "    # we use eigh becuase C is symmetric (faster runtime)\n",
    "    e_vals, e_vecs = np.linalg.eigh(C)\n",
    "    k_pcs = e_vecs[:,-comps:]\n",
    "\n",
    "    return k_pcs\n",
    "\n",
    "def get_folds(data, k):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Returns the indices from the data set of training and test data\n",
    "                for k-folds cross validation.\n",
    "    Parameters:  \n",
    "                data: array_like\n",
    "                   Data set to split.\n",
    "                k: int\n",
    "                    Number of folds.\n",
    "    Returns:\n",
    "                folds: list\n",
    "                    Contains a list of array pairs <train, test> that are the \n",
    "                    indicies from the data set for training and testing.\n",
    "    \"\"\"\n",
    "    folds = []\n",
    "    \n",
    "    size = int(data.shape[0]/k) # size of each fold\n",
    "    for i in range(0, data.shape[0], size):\n",
    "        train = list(range(i)) + list(range(i+size, data.shape[0]))\n",
    "        test  = list(range(i, i+size))\n",
    "        folds.append((train, test))\n",
    "    \n",
    "    return folds\n",
    "\n",
    "def getData(X, y, classes):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Returns data for specific classes indicated from provided data.\n",
    "    Parameters:  \n",
    "                X: array_like\n",
    "                    The original dataset.\n",
    "                y: array_like\n",
    "                    The labels for each data point in the dataset.\n",
    "                classes: array_like\n",
    "                    Labels of the classes desired.\n",
    "    Returns:\n",
    "                new_data:\n",
    "                    Data for the classes specified by classes.\n",
    "                new_labels:\n",
    "                    Labels for the classes specified by classes.\n",
    "    \"\"\"\n",
    "    new_data   = []\n",
    "    new_labels = []\n",
    "\n",
    "    for label in classes:\n",
    "        # see the following link for more information: https://numpy.org/doc/stable/reference/generated/numpy.argwhere.html\n",
    "        # .T[0] is because argwhere() returns a column vector and we need normal array to index data\n",
    "        idxs = np.argwhere(y==label).T[0]\n",
    "        new_data.append(X[idxs])\n",
    "        new_labels.append(y[idxs])\n",
    "    \n",
    "    new_data   = np.vstack(new_data)\n",
    "    new_labels = np.hstack(new_labels)\n",
    "\n",
    "    return new_data, new_labels\n",
    "\n",
    "def prepareData(X, k_pcs, fold=None, test=False):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Prepares the data for training by performing k mutex splits, \n",
    "                dimensionality reduction using PCA, and adding a bias term to the data. \n",
    "    Parameters:  \n",
    "                X: array_like\n",
    "                    The original dataset.\n",
    "                k_pcs: array_like\n",
    "                    The top k principal components of the dataset.\n",
    "    Returns:\n",
    "                train_set: array_like\n",
    "                    Data ready for training.\n",
    "                val_set: array_like\n",
    "                    Data ready for validation testing.\n",
    "    \"\"\"\n",
    "    if test:\n",
    "        print(X.shape, \"prepare\")\n",
    "        test_set = X@k_pcs # get training, reduce using PCA\n",
    "        test_set = np.hstack((np.ones((test_set.shape[0], 1)), test_set)) # add bias\n",
    "        return test_set\n",
    "    \n",
    "    train_set = X[fold[0]]@k_pcs # get training, reduce using PCA\n",
    "    train_set = np.hstack((np.ones((train_set.shape[0], 1)), train_set)) # add bias\n",
    "    \n",
    "    val_set   = X[fold[1]]@k_pcs # get validation, reduce using PCA\n",
    "    val_set   = np.hstack((np.ones((val_set.shape[0], 1)), val_set)) # add bias\n",
    "    \n",
    "    return train_set, val_set\n",
    "\n",
    "def predictMulti(X, W):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Used for multiclass prediction.\n",
    "                Returns an array of predictions for data X with weights W. \n",
    "    Parameters:  \n",
    "                X: array_like\n",
    "                    Dataset to make predictions on.\n",
    "                W: array_like\n",
    "                    The weights of the model.\n",
    "    Returns:\n",
    "                y_hat: array_like.\n",
    "                    An array of prediction on X.\n",
    "    \"\"\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "def predict(X, W):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Returns an array of predictions for data X with weights W. \n",
    "    Parameters:  \n",
    "                X: array_like\n",
    "                    Dataset to make predictions on.\n",
    "                W: array_like\n",
    "                    The weights of the model.\n",
    "    Returns:\n",
    "                y_hat: array_like.\n",
    "                    An array of prediction on X.\n",
    "                G: array_like\n",
    "                    Array of probabilities for each class.\n",
    "    \"\"\"\n",
    "    y_hat = []\n",
    "    \n",
    "    if W.shape[1] > 1:\n",
    "        return predictMulti(X, W)\n",
    "    \n",
    "    A = X@W\n",
    "    G = 1.0 / (np.exp(-A)+1)\n",
    "    y_hat = np.asarray([1 if y >= 0.5 else 0 for y in G])\n",
    "    \n",
    "    return y_hat, G\n",
    "\n",
    "def SGD(X, y, y_hat, W, lr, batchSize):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Estimates a new weights matrix W_new using Stochasitc Gradient Descent.\n",
    "    Parameters:  \n",
    "                X: array_like\n",
    "                    Training examples.\n",
    "                y: array_like\n",
    "                    Labels for each training example in X.\n",
    "                y_hat: array_like\n",
    "                    Probablities for each class for each training example in X.\n",
    "                W: array_like\n",
    "                    Current weights matrix for the model.\n",
    "                lr: int\n",
    "                    Learning rate\n",
    "                batchSize:\n",
    "                    Batch size for mini-batching\n",
    "    Returns:\n",
    "                W_new: array_like.\n",
    "                     A new estimation of the weights matrix.\n",
    "    \"\"\"\n",
    "    W_new = W\n",
    "    randIdxs = np.random.rand(X.shape[0]).argsort()\n",
    "    \n",
    "    for i in range(0, X.shape[0], batchSize):\n",
    "        start = i\n",
    "        end   = min(i+batchSize, X.shape[0])\n",
    "\n",
    "        gradient = ((y[randIdxs[start:end]] - y_hat[randIdxs[start:end]]).T@X[randIdxs[start:end]]).reshape((-1,1))\n",
    "        W_new = W_new + lr*gradient\n",
    "        \n",
    "    return W_new\n",
    "\n",
    "def crossEntropy(y, y_hat):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Computes the Cross-Entropy loss.\n",
    "    Parameters:  \n",
    "                y: array_like\n",
    "                    Labels for each training example in the dataset.\n",
    "                y_hat: array_like\n",
    "                    Probablities for each class for each training example in the dataset.\n",
    "    Returns:\n",
    "                loss: float.\n",
    "                     The loss.\n",
    "    \"\"\"\n",
    "    offset = 10**-8 # to prevent log(0)\n",
    "    loss   = np.dot(y.flatten(), np.log(y_hat.flatten()+offset)) + np.dot( (1-y.flatten()), np.log(1-y_hat.flatten()+offset) )\n",
    "    \n",
    "    return -loss\n",
    "\n",
    "def plot(losses, titles):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                Plots the losses in losses with corresponding title from title.\n",
    "    Parameters:  \n",
    "                losses: list\n",
    "                    Losses to plot.\n",
    "                titles: list\n",
    "                    Title for each loss.\n",
    "    Returns:\n",
    "                Nothing\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    for i in range(len(losses)):\n",
    "        plt.plot(list(range(1, len(losses[i])+1)), losses[i], label=titles[i])\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def testModel(X, y, model, pcs, classes):\n",
    "    X_test, y_test = getData(X, y, classes)\n",
    "    X_test = prepareData(X_test, pcs, test=True)\n",
    "    y_test = one_hot_encode(y_test)\n",
    "    \n",
    "    \n",
    "    predictions, _ = predict(X_test, model)\n",
    "    total = y_test.flatten().shape[0]\n",
    "    numWrong = abs(np.sum(y_test.flatten()-predictions.flatten()))\n",
    "    print(np.sum(y_test.flatten()), \"test\")\n",
    "    print(np.sum(predictions.flatten()), \"pred\")\n",
    "    \n",
    "    acc = (total - numWrong)/total\n",
    "    \n",
    "    return acc\n",
    "    \n",
    "def logisticRegression(data, labels, classes):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "                TODO Trains a prediction model using x_train, y_train\n",
    "    Parameters:  \n",
    "                data: array_like\n",
    "                    Training examples.\n",
    "                labels: array_like\n",
    "                    Labels of the training examples.\n",
    "    Returns:\n",
    "                model: array_like\n",
    "                    The best model from training used for prediction.\n",
    "    \"\"\"\n",
    "    LR = .01 # Learning rate\n",
    "    BS = 512 # Batch Size\n",
    "    p  = 50  # top p PC's\n",
    "    k  = 10   # number of folds\n",
    "    M  = 100 # number of epochs\n",
    "    loss    = crossEntropy # alias crossEntropy function\n",
    "    model   = None # best model from training\n",
    "    loss_val   = []\n",
    "    loss_train = []\n",
    "    \n",
    "    X, y = getData(data, labels, classes)\n",
    "    X, y = shuffle(X, y)\n",
    "    y = one_hot_encode(y)\n",
    "\n",
    "    folds = get_folds(X, k=k)\n",
    "    k_pcs = pca(data, comps=p)\n",
    "    \n",
    "    # initialize weights matrix for a given number\n",
    "    # of classes and PC's\n",
    "    if len(classes) <= 2:\n",
    "        W = np.random.rand(p+1, 1)\n",
    "    else:\n",
    "        W = np.random.rand(p+1, len(classes)) # initialize weights\n",
    "    \n",
    "    for fold in tqdm(folds):\n",
    "\n",
    "        train_set, val_set = prepareData(X, k_pcs, fold)\n",
    "        y_train = y[fold[0]]\n",
    "        y_val   = y[fold[1]]\n",
    "        \n",
    "        for epoch in range(M):\n",
    "            prediction_train, y_hat_train = predict(train_set, W)\n",
    "            loss_train.append(loss(y_train, y_hat_train))\n",
    "            \n",
    "            prediction_val, y_hat_val = predict(val_set, W)\n",
    "            loss_val.append(loss(y_val, y_hat_val))\n",
    "            \n",
    "            W = SGD(train_set, y_train, y_hat_train, W, LR, BS)\n",
    "            \n",
    "    plot([loss_train, loss_val], [\"Training Loss\", \"Validation Loss\"])\n",
    "    \n",
    "    return W, k_pcs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]/home/darylnak/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:226: RuntimeWarning: overflow encountered in exp\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.79s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 784) prepare\n",
      "1000 test\n",
      "1000 pred\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    classes = [0, 9] # classes we want for binary classification\n",
    "    X_train, y_train = mnist_reader.load_mnist('fashion_mnist_dataset/data/fashion', kind='train')\n",
    "    X_test, y_test   = mnist_reader.load_mnist('fashion_mnist_dataset/data/fashion', kind='t10k')\n",
    "    \n",
    "    # min-max normalize\n",
    "    X_train = normalize(X_train)\n",
    "    X_test  = normalize(X_test)\n",
    "    \n",
    "    model, pcs = logisticRegression(X_train, y_train, classes)\n",
    "    acc = testModel(X_test, y_test, model, pcs, classes)\n",
    "    print(acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
